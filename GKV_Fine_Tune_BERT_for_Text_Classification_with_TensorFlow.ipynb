{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GKV_Fine-Tune-BERT-for-Text-Classification-with-TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkv856/KaggleData/blob/main/GKV_Fine_Tune_BERT_for_Text_Classification_with_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGCJYkQj_Uu2"
      },
      "source": [
        "<h2 align=center> Fine-Tune BERT for Text Classification with TensorFlow</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y2m1S6e12il"
      },
      "source": [
        "<div align=\"center\">\n",
        "    <img width=\"512px\" src='https://drive.google.com/uc?id=1fnJTeJs5HUpz7nix-F9E6EZdgUflqyEu' />\n",
        "    <p style=\"text-align: center;color:gray\">Figure 1: BERT Classification Model</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYYYWqWr_WCC"
      },
      "source": [
        "In this [project](https://www.coursera.org/projects/fine-tune-bert-tensorflow/), you will learn how to fine-tune a BERT model for text classification using TensorFlow and TF-Hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yQG5PCO_WFx"
      },
      "source": [
        "The pretrained BERT model used in this project is [available](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2) on [TensorFlow Hub](https://tfhub.dev/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pKNS21u_WJo"
      },
      "source": [
        "### Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3NHSMXv_WMv"
      },
      "source": [
        "By the time you complete this project, you will be able to:\n",
        "\n",
        "- Build TensorFlow Input Pipelines for Text Data with the [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) API\n",
        "- Tokenize and Preprocess Text for BERT\n",
        "- Fine-tune BERT for text classification with TensorFlow 2 and [TF Hub](https://tfhub.dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6BEe-3-AVRQ"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc9f-8rLAVUS"
      },
      "source": [
        "In order to be successful with this project, it is assumed you are:\n",
        "\n",
        "- Competent in the Python programming language\n",
        "- Familiar with deep learning for Natural Language Processing (NLP)\n",
        "- Familiar with TensorFlow, and its Keras API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYXXV5n3Ab-4"
      },
      "source": [
        "### Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhK-SYGyAjxe"
      },
      "source": [
        "This project/notebook consists of several Tasks.\n",
        "\n",
        "- **[Task 1]()**: Introduction to the Project.\n",
        "- **[Task 2]()**: Setup your TensorFlow and Colab Runtime\n",
        "- **[Task 3]()**: Download and Import the Quora Insincere Questions Dataset\n",
        "- **[Task 4]()**: Create tf.data.Datasets for Training and Evaluation\n",
        "- **[Task 5]()**: Download a Pre-trained BERT Model from TensorFlow Hub\n",
        "- **[Task 6]()**: Tokenize and Preprocess Text for BERT\n",
        "- **[Task 7]()**: Wrap a Python Function into a TensorFlow op for Eager Execution\n",
        "- **[Task 8]()**: Create a TensorFlow Input Pipeline with `tf.data`\n",
        "- **[Task 9]()**: Add a Classification Head to the BERT `hub.KerasLayer`\n",
        "- **[Task 10]()**: Fine-Tune BERT for Text Classification\n",
        "- **[Task 11]()**: Evaluate the BERT Text Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaArqXjRAcBa"
      },
      "source": [
        "## Task 2: Setup your TensorFlow and Colab Runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDDhjzZ5A4Q_"
      },
      "source": [
        "You will only be able to use the Colab Notebook after you save it to your Google Drive folder. Click on the File menu and select “Save a copy in Drive…\n",
        "\n",
        "![Copy to Drive](https://drive.google.com/uc?id=1CH3eDmuJL8WR0AP1r3UE6sOPuqq8_Wl7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpe6GhLuBJWB"
      },
      "source": [
        "### Check GPU Availability\n",
        "\n",
        "Check if your Colab notebook is configured to use Graphical Processing Units (GPUs). If zero GPUs are available, check if the Colab notebook is configured to use GPUs (Menu > Runtime > Change Runtime Type).\n",
        "\n",
        "![Hardware Accelerator Settings](https://drive.google.com/uc?id=1qrihuuMtvzXJHiRV8M7RngbxFYipXKQx)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V9c8vzSL3aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b1a480-702c-41a5-c7d6-cdaf43cc1735"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obch3rAuBVf0"
      },
      "source": [
        "### Install TensorFlow and TensorFlow Model Garden"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUQEY3dFB0jX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72dfa7f2-3ebe-45ce-a438-df9386eb1714"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.version.VERSION)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU3YLZ1TYKUt"
      },
      "source": [
        "# !pip install -q tensorflow==2.3.0"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFRTC-zwUy6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe64046-1754-4847-be97-39ae780b9889"
      },
      "source": [
        "!git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H2G0571zLLs"
      },
      "source": [
        "# install requirements to use tensorflow/models repository\n",
        "# !pip install -Uqr models/official/requirements.txt\n",
        "# you may have to restart the runtime afterwards"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVjksk4yCXur"
      },
      "source": [
        "## Restart the Runtime\n",
        "\n",
        "**Note** \n",
        "After installing the required Python packages, you'll need to restart the Colab Runtime Engine (Menu > Runtime > Restart runtime...)\n",
        "\n",
        "![Restart of the Colab Runtime Engine](https://drive.google.com/uc?id=1xnjAy2sxIymKhydkqb0RKzgVK9rh3teH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMsEoT3Fg4Wg"
      },
      "source": [
        "## Task 3: Download and Import the Quora Insincere Questions Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmqEylyFYTdP"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import sys\n",
        "sys.path.append('models')"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVQOO_x-XFF_",
        "outputId": "7e4f5e54-916e-4b02-a67d-f991722bae2b"
      },
      "source": [
        "!pip install tensorflow_text\n",
        "!pip install sentencepiece \n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "from helper_functions import *"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: tensorflow<2.7,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.39.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.37.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (5.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.5)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.34.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.5.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "--2021-09-17 16:21:24--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py.2’\n",
            "\n",
            "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-09-17 16:21:24 (75.9 MB/s) - ‘helper_functions.py.2’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq38DSteXbj8",
        "outputId": "17ad2adb-4d70-4fb6-fe0b-a853602a476b"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlMwHdkmXThB"
      },
      "source": [
        "sys.path.append('models')\n",
        "from official.nlp.data import classifier_data_lib\n",
        "from official.nlp.bert import tokenization\n",
        "from official.nlp import optimization"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56XE9KHTXZQF",
        "outputId": "b5e617a5-9d93-4d18-ce40-67f1bd2b7756"
      },
      "source": [
        "!pip install fsspec"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (2021.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuX1lB8pPJ-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8cfa67-da83-475b-9697-fa4b250d60fb"
      },
      "source": [
        "print(\"TF Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF Version:  2.6.0\n",
            "Eager mode:  True\n",
            "Hub version:  0.12.0\n",
            "GPU is NOT AVAILABLE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtbwpWgyEZg7"
      },
      "source": [
        "A downloadable copy of the [Quora Insincere Questions Classification data](https://www.kaggle.com/c/quora-insincere-questions-classification/data) can be found [https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip](https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip). Decompress and read the data into a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nI-9itVwCCQ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# DATA_URL = \"https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip\"\n",
        "# df = pd.read_csv(DATA_URL, compression=\"zip\", low_memory=False)\n",
        "\n",
        "DATA_URL = \"https://raw.githubusercontent.com/gkv856/KaggleData/main/train.csv\"\n",
        "df = pd.read_csv(DATA_URL, low_memory=False)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "IaIgaA4lXoOH",
        "outputId": "f4ea4c83-6f82-43e7-fbe0-f806a15dfacc"
      },
      "source": [
        "df.tail(20)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7593</th>\n",
              "      <td>10848</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I just heard a really loud bang and everyone i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7594</th>\n",
              "      <td>10849</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A gas thing just exploded and I heard screams ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7595</th>\n",
              "      <td>10850</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NWS: Flash Flood Warning Continued for Shelby ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7596</th>\n",
              "      <td>10851</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @LivingSafely: #NWS issues Severe #Thunders...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7597</th>\n",
              "      <td>10852</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#??? #?? #??? #??? MH370: Aircraft debris foun...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7598</th>\n",
              "      <td>10853</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Father-of-three Lost Control of Car After Over...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7599</th>\n",
              "      <td>10854</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.3 #Earthquake in 9Km Ssw Of Anza California ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7600</th>\n",
              "      <td>10855</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Evacuation order lifted for town of Roosevelt:...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7601</th>\n",
              "      <td>10859</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#breaking #LA Refugio oil spill may have been ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7602</th>\n",
              "      <td>10860</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>a siren just went off and it wasn't the Forney...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7603</th>\n",
              "      <td>10862</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Officials say a quarantine is in place at an A...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7604</th>\n",
              "      <td>10863</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7605</th>\n",
              "      <td>10864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7606</th>\n",
              "      <td>10866</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7607</th>\n",
              "      <td>10867</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#stormchase Violent Record Breaking EF-5 El Re...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7608</th>\n",
              "      <td>10869</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7609</th>\n",
              "      <td>10870</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7610</th>\n",
              "      <td>10871</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7611</th>\n",
              "      <td>10872</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Police investigating after an e-bike collided ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7612</th>\n",
              "      <td>10873</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id keyword  ...                                               text target\n",
              "7593  10848     NaN  ...  I just heard a really loud bang and everyone i...      0\n",
              "7594  10849     NaN  ...  A gas thing just exploded and I heard screams ...      1\n",
              "7595  10850     NaN  ...  NWS: Flash Flood Warning Continued for Shelby ...      1\n",
              "7596  10851     NaN  ...  RT @LivingSafely: #NWS issues Severe #Thunders...      1\n",
              "7597  10852     NaN  ...  #??? #?? #??? #??? MH370: Aircraft debris foun...      1\n",
              "7598  10853     NaN  ...  Father-of-three Lost Control of Car After Over...      1\n",
              "7599  10854     NaN  ...  1.3 #Earthquake in 9Km Ssw Of Anza California ...      1\n",
              "7600  10855     NaN  ...  Evacuation order lifted for town of Roosevelt:...      1\n",
              "7601  10859     NaN  ...  #breaking #LA Refugio oil spill may have been ...      1\n",
              "7602  10860     NaN  ...  a siren just went off and it wasn't the Forney...      1\n",
              "7603  10862     NaN  ...  Officials say a quarantine is in place at an A...      1\n",
              "7604  10863     NaN  ...  #WorldNews Fallen powerlines on G:link tram: U...      1\n",
              "7605  10864     NaN  ...  on the flip side I'm at Walmart and there is a...      1\n",
              "7606  10866     NaN  ...  Suicide bomber kills 15 in Saudi security site...      1\n",
              "7607  10867     NaN  ...  #stormchase Violent Record Breaking EF-5 El Re...      1\n",
              "7608  10869     NaN  ...  Two giant cranes holding a bridge collapse int...      1\n",
              "7609  10870     NaN  ...  @aria_ahrary @TheTawniest The out of control w...      1\n",
              "7610  10871     NaN  ...  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...      1\n",
              "7611  10872     NaN  ...  Police investigating after an e-bike collided ...      1\n",
              "7612  10873     NaN  ...  The Latest: More Homes Razed by Northern Calif...      1\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeHE98KiMvDd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "ff98190b-d310-49b5-a9c0-6a4aa9705291"
      },
      "source": [
        "# our data is imbalanced and we will assume that in the real world/test data, \n",
        "# this will continued to be the case \n",
        "df[\"target\"].plot(kind=\"hist\")"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7826038a10>"
            ]
          },
          "metadata": {},
          "execution_count": 161
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leRFRWJMocVa"
      },
      "source": [
        ""
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELjswHcFHfp3"
      },
      "source": [
        "## Task 4: Create tf.data.Datasets for Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fScULIGPwuWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6591c267-06f1-49df-b90d-f12967d72f41"
      },
      "source": [
        "# later you can use the whole data for traiing and test\n",
        "train_df, valid_df = train_test_split(df, \n",
        "                                       random_state=42, \n",
        "                                      #  train_size=0.9, \n",
        "                                       test_size=.3,\n",
        "                                       stratify=df[\"target\"].values)\n",
        "\n",
        "# use the below structure for testing if the data is huge\n",
        "# train_df, remaining = train_test_split(df, \n",
        "#                                        random_state=42, \n",
        "#                                        train_size=0.0095, \n",
        "#                                        stratify=df[\"target\"].values)\n",
        "\n",
        "# valid_df, _ = train_test_split(remaining, \n",
        "#                               random_state=42, \n",
        "#                               train_size=0.0095, \n",
        "#                               stratify=remaining[\"target\"].values)\n",
        "len(train_df), len(valid_df)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72, 71)"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQYMGT5_qLPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deaf1d71-0f63-4190-931b-1a21974540a8"
      },
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  train_data = tf.data.Dataset.from_tensor_slices((train_df[\"text\"].values,\n",
        "                                                   train_df[\"target\"].values))\n",
        "  \n",
        "  test_data = tf.data.Dataset.from_tensor_slices((valid_df[\"text\"].values,\n",
        "                                                   valid_df[\"target\"].values))\n",
        "train_data, test_data  "
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
              " <TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)>)"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo0ygWU5bYFF",
        "outputId": "bae9634c-58e9-4174-99ba-34448627d9bc"
      },
      "source": [
        "for text, label in train_data.take(1):\n",
        "  print(text)\n",
        "  print(label)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b\"I had school today and I've already had a panic attack. Thank you high school for sucking !!!\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2-ReN88Hvy_"
      },
      "source": [
        "## Task 5: Download a Pre-trained BERT Model from TensorFlow Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMb5M86b4-BU"
      },
      "source": [
        "\"\"\"\n",
        "Each line of the dataset is composed of the review text and its label\n",
        "- Data preprocessing consists of transforming text to BERT input features:\n",
        "input_word_ids, input_mask, segment_ids\n",
        "- In the process, tokenizing the text is done with the provided BERT model tokenizer\n",
        "\"\"\"\n",
        "\n",
        "# Label categories, right now our data has these categories\n",
        "label_list = [0, 1]\n",
        "\n",
        "# maximum length of (token) input sequences, or the words in a question\n",
        "# to save speed we should reset this\n",
        "max_seq_length = 64\n",
        "\n",
        "train_batch_size = 32\n",
        "\n",
        "# Get BERT layer and tokenizer:\n",
        "# More details here: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\n",
        "\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",\n",
        "                            trainable=True)\n",
        "\n",
        "\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QinzNq6OsP1"
      },
      "source": [
        "## Task 6: Tokenize and Preprocess Text for BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FTqJ698zZ1e"
      },
      "source": [
        "<div align=\"center\">\n",
        "    <img width=\"512px\" src='https://drive.google.com/uc?id=1-SpKFELnEvBMBqO7h3iypo8q9uUUo96P' />\n",
        "    <p style=\"text-align: center;color:gray\">Figure 2: BERT Tokenizer</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWYkggYe6HZc"
      },
      "source": [
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create InputExamples using `classifier_data_lib`'s constructor `InputExample` provided in the BERT library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-21A5aNJM0W"
      },
      "source": [
        "# This provides a function to convert row to input features and label\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "def get_clean_text(text_str):\n",
        "  text_str = BeautifulSoup(text_str, \"lxml\").get_text()\n",
        "  text_str = re.sub(r\"@[A-Za-z0-9]+\", \" \", text_str)\n",
        "  text_str = re.sub(r\"https?://[[A-Za-z0-9./]+\", \" \", text_str)\n",
        "  text_str = re.sub(r\"[^a-zA-Z.!?']\", \" \", text_str)\n",
        "  text_str = re.sub(r\" +\", \" \", text_str)\n",
        "  # text_str = text_str.lower()\n",
        "  # text_str = replace_typical_misspell(text_str)\n",
        "\n",
        "  return text_str\n",
        "\n",
        "def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):\n",
        "  # guid is unique id for each example, we dont need textb as per our usecase\n",
        "  clean_text = text.numpy()#get_clean_text(text.numpy())\n",
        "  example = classifier_data_lib.InputExample(guid=None, text_a=clean_text, label=label.numpy())\n",
        "  \n",
        "  feature = classifier_data_lib.convert_single_example(0, \n",
        "                                                       example, \n",
        "                                                       label_list, \n",
        "                                                       max_seq_length, \n",
        "                                                       tokenizer)\n",
        "  # print(feature)\n",
        "  # input_ids,\n",
        "  #              input_mask,\n",
        "  #              segment_ids,\n",
        "  return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)\n",
        "  "
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_HQSsHwWCsK"
      },
      "source": [
        "You want to use [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) to apply this function to each element of the dataset. [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) runs in graph mode.\n",
        "\n",
        "- Graph tensors do not have a value.\n",
        "- In graph mode you can only use TensorFlow Ops and functions.\n",
        "\n",
        "So you can't `.map` this function directly: You need to wrap it in a [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function). The [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaNlkKVfWX0Q"
      },
      "source": [
        "## Task 7: Wrap a Python Function into a TensorFlow op for Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGACBcfCWC2O"
      },
      "source": [
        "def to_feature_map(text, label):\n",
        "  out = tf.py_function(to_feature, inp=[text, label], \n",
        "                       Tout=[tf.int32, tf.int32, tf.int32, tf.int32,])\n",
        "  iids, imask, segids, label = out[0], out[1], out[2], out[3]\n",
        "\n",
        "  iids.set_shape([max_seq_length])\n",
        "  imask.set_shape([max_seq_length])\n",
        "  segids.set_shape([max_seq_length])\n",
        "  label.set_shape([])\n",
        "\n",
        "  x = {\n",
        "      \"input_word_ids\": iids,\n",
        "       \"input_mask\": imask,\n",
        "       \"input_type_ids\": segids\n",
        "  }\n",
        "  # x = text\n",
        "  return (x, label)\n",
        "\n"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhdO6MjTbtn1"
      },
      "source": [
        "## Task 8: Create a TensorFlow Input Pipeline with `tf.data`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv7EX3BSm-ak",
        "outputId": "77a1560d-2a63-4f4a-fd11-5be0f8f9d798"
      },
      "source": [
        "s = train_data.take(1)\n",
        "for t, l in s:\n",
        "  print(to_feature_map(t, l))"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_word_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
            "array([  101,  1045,  2018,  2082,  2651,  1998,  1045,  1005,  2310,\n",
            "        2525,  2018,  1037,  6634,  2886,  1012,  4067,  2017,  2152,\n",
            "        2082,  2005, 13475,   999,   999,   999,   102,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32)>, 'input_mask': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
            "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>, 'input_type_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
            "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>}, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHRdiO3dnPNr"
      },
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  # train\n",
        "  train_data = (train_data\n",
        "                .map(to_feature_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .shuffle(1000)\n",
        "                .batch(train_batch_size, drop_remainder=True)\n",
        "                .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "  # valid\n",
        "  \n",
        "  test_data = (test_data\n",
        "                .map(to_feature_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                # .shuffle(1000) we dont need to shuffle the test data, just need them for predictions\n",
        "                .batch(train_batch_size, drop_remainder=True)\n",
        "                .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "  "
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLUWnfx-YDi2"
      },
      "source": [
        "The resulting `tf.data.Datasets` return `(features, labels)` pairs, as expected by [`keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Z2cy9GHQ8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7667e5-44f3-4c3a-9c03-f0a66b579f67"
      },
      "source": [
        "# train data spec\n",
        "train_data.element_spec"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_mask': TensorSpec(shape=(32, 128), dtype=tf.int32, name=None),\n",
              "  'input_type_ids': TensorSpec(shape=(32, 128), dtype=tf.int32, name=None),\n",
              "  'input_word_ids': TensorSpec(shape=(32, 128), dtype=tf.int32, name=None)},\n",
              " TensorSpec(shape=(32,), dtype=tf.int32, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGAH-ycYOmao"
      },
      "source": [
        "# valid data spec\n"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZxe-7yhPyQe"
      },
      "source": [
        "## Task 9: Add a Classification Head to the BERT Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9THH5V0Dw2HO"
      },
      "source": [
        "<div align=\"center\">\n",
        "    <img width=\"512px\" src='https://drive.google.com/uc?id=1fnJTeJs5HUpz7nix-F9E6EZdgUflqyEu' />\n",
        "    <p style=\"text-align: center;color:gray\">Figure 3: BERT Layer</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQjnFJ6_rtdo"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "# Train model\n",
        "from datetime import datetime\n",
        "from tensorflow.keras import callbacks"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9il4gtlADcp"
      },
      "source": [
        "# Building the model\n",
        "def create_model(drop_rate=0.4, m_name=\"model\"):\n",
        "  input_word_ids = layers.Input(shape=(max_seq_length, ),\n",
        "                             dtype=tf.int32,\n",
        "                             name=\"input_word_ids\")\n",
        "  input_mask = layers.Input(shape=(max_seq_length, ),\n",
        "                             dtype=tf.int32,\n",
        "                             name=\"input_mask\")\n",
        "  segment_ids = layers.Input(shape=(max_seq_length, ),\n",
        "                             dtype=tf.int32,\n",
        "                             name=\"segment_ids\")\n",
        "  # pooled output is vector representation of the whole sentence\n",
        "  # sequesnced output is vector representation of each word\n",
        "  pooled_out, sequenced_out = bert_layer([input_word_ids, input_mask, segment_ids],)\n",
        "\n",
        "\n",
        "  # x = layers.Dense(512, activation = 'relu')(pooled_out)\n",
        "  # x = layers.Dropout(0.3)(x)\n",
        "  # x = layers.Dense(256, activation = 'relu')(x)\n",
        "  # x = layers.Dropout(0.2)(x)\n",
        "  x = layers.Dense(64, activation = 'relu')(pooled_out)\n",
        "\n",
        "  dropout = layers.Dropout(drop_rate)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\", name=\"outputs\")(dropout)\n",
        "\n",
        "  model = tf.keras.Model(\n",
        "      #mapping the input dict values here\n",
        "      inputs = {\n",
        "            'input_word_ids': input_word_ids,\n",
        "            'input_mask': input_mask,\n",
        "            'input_type_ids': segment_ids,\n",
        "      }, \n",
        "      outputs=outputs, name=m_name)\n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H1uja6U8drM"
      },
      "source": [
        "def compile_model(model, lr=2e-5):\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "  model_fig = tf.keras.utils.plot_model(model, show_shapes=True, dpi=72)\n",
        "  return model, model_fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0FsLELP8fu0"
      },
      "source": [
        "model = create_model(m_name=\"model0\")\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxzj5dWa8iDf"
      },
      "source": [
        "model, _ = compile_model(model, 1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW85Nb_I8kvm"
      },
      "source": [
        "now = datetime.now()\n",
        "curr_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "filepath = \"/content/drive/MyDrive/AI_ML_DL/Kaggle/01. Disaster tweet/dense\" + curr_time  \n",
        "chk_pt = callbacks.ModelCheckpoint(filepath=filepath,\n",
        "                                    save_weights_only=True,\n",
        "                                    verbose=0)\n",
        "\n",
        "EPOCHS = 3\n",
        "history = model.fit(train_data,\n",
        "                    validation_data=test_data,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[chk_pt])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6maM-vr7YaJ"
      },
      "source": [
        "## Task 10: Fine-Tune BERT for Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNZl1lx_cA5Y"
      },
      "source": [
        "## Task 11: Evaluate the BERT Text Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCjgrUYH_IsE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "  plt.show()"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opu9neBA_98R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "52f23b96-ba01-480b-808d-70522deede38"
      },
      "source": [
        "plot_graphs(history, \"accuracy\")\n",
        "y_pred = model.predict(test_data)\n",
        "calculate_results(valid_df[\"target\"], y_pred)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zULshvwAPZ7"
      },
      "source": [
        "\n",
        "TEST_URL = \"https://raw.githubusercontent.com/gkv856/KaggleData/main/test.csv\"\n",
        "df_test = pd.read_csv(TEST_URL)\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5C64CT-clSG",
        "outputId": "7095e6d1-f29d-465c-d40c-b105e82707dd"
      },
      "source": [
        "# num = 5\n",
        "# test_sample = df_test[\"text\"].head(num)\n",
        "# ids = df_test['id'].head(num)\n",
        "# test_sample\n",
        "\n",
        "num = 5\n",
        "test_sample = df_test[\"text\"]#.head(num)\n",
        "ids = df_test['id']#.head(num)\n",
        "test_sample"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   Just happened a terrible car crash\n",
              "1    Heard about #earthquake is different cities, s...\n",
              "2    there is a forest fire at spot pond, geese are...\n",
              "3             Apocalypse lighting. #Spokane #wildfires\n",
              "4        Typhoon Soudelor kills 28 in China and Taiwan\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "V1vRrhPRDC5d",
        "outputId": "fd3a0a61-dcff-4787-9892-8fea70354a1e"
      },
      "source": [
        "real_test_data = tf.data.Dataset.from_tensor_slices((test_sample, [0]*len(test_sample)))\n",
        "real_test_data = (real_test_data.map(to_feature_map).batch(train_batch_size)\n",
        "                  .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "y_preds = model.predict(real_test_data) \n",
        "y_preds = tf.round(y_preds)\n",
        "y_preds = tf.cast(y_preds, dtype=tf.int32)\n",
        "\n",
        "res = tf.squeeze(y_preds).numpy()\n",
        "my_sub = pd.DataFrame({'id': ids,'target':res})\n",
        "\n",
        "my_sub.head(20)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  target\n",
              "0   0       1\n",
              "1   2       1\n",
              "2   3       1\n",
              "3   9       1\n",
              "4  11       1"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0HEFkfdDC9Y"
      },
      "source": [
        "my_sub.to_csv(\"sub.csv\", index=False)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBs5vj0Siz-2"
      },
      "source": [
        "model3 = create_model(m_name=\"model3\")"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTDnUcgImoxt",
        "outputId": "0191f091-dbd5-426f-ec62-e82cfa103acb"
      },
      "source": [
        "model3, _ = compile_model(model3, lr=0.1)\n",
        "model3.load_weights(filepath)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7829f0f650>"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loG1HH3nm1HS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}