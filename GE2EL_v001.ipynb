{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GE2EL_v001.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1EFBQbNIum8vg8Uq7kjav-9cgLNpYx02X",
      "authorship_tag": "ABX9TyOaOKj2CdLt5bQlMsroBFz4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkv856/KaggleData/blob/main/GE2EL_v001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv2W1KVYeNeq"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras.layers import LSTM, Input\n",
        "from tensorflow import Variable\n",
        "import tensorflow.keras.layers as layers"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ijxuGkAFZgN",
        "outputId": "37f2a1cc-0931-4dc0-ac19-9cace88f7fd7"
      },
      "source": [
        "!git clone https://github.com/gkv856/gkv856-GE2EL_tensorflow_implementation_with_data.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gkv856-GE2EL_tensorflow_implementation_with_data'...\n",
            "remote: Enumerating objects: 429, done.\u001b[K\n",
            "remote: Counting objects: 100% (429/429), done.\u001b[K\n",
            "remote: Compressing objects: 100% (419/419), done.\u001b[K\n",
            "remote: Total 429 (delta 7), reused 428 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (429/429), 87.47 MiB | 24.42 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBQlCE4UGlF8"
      },
      "source": [
        "from gkv.string_constants.configuration_file import *\n",
        "from gkv.gkv_implement.util import *"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mhuuOXT9EHt"
      },
      "source": [
        "config[\"train_path\"] = \"/content/gkv/audio_data/spectrograms/train\"\n",
        "config[\"embed_dim\"] = 128\n",
        "config[\"lr\"] = 1e-6\n",
        "\n",
        "# config"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxhJm9J4J4jQ"
      },
      "source": [
        "def create_model(inp_shape, m_name=\"model\"):\n",
        "\n",
        "    # input batch (time x batch x n_mel)\n",
        "    inputs = layers.Input(shape=inp_shape, dtype=tf.float32)\n",
        "    x = inputs\n",
        "    x = layers.LSTM(units=config[\"embed_dim\"], return_sequences=True, name=\"l1\")(x)\n",
        "    x = layers.LSTM(units=config[\"embed_dim\"], return_sequences=True, name=\"l2\")(x)\n",
        "    x = layers.LSTM(units=config[\"n_mels\"], dtype=tf.float32, return_sequences=True, return_state=True, name=\"l3\")(x)\n",
        "    # embedded = x[-1]\n",
        "    # norm_embedding = normalize(x)\n",
        "    # for y in x:\n",
        "    #   print(\"norm_embedding embedded size: \", y)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs=x, name=m_name)\n",
        "    return model\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W71odW5BFYkF"
      },
      "source": [
        "# len_utterence = None # since each batch will have different length\n",
        "inp_shape = [config[\"speaker_num\"]*config[\"utter_num\"], config[\"n_mels\"]]\n",
        "model = create_model(inp_shape, \"model0\")\n",
        "# model.summary()"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3anZa6ogzxOq"
      },
      "source": [
        "model_optimizer = get_optimizer(lr=config[\"lr\"])\n",
        "from gkv.gkv_implement.util import *"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz3Q_iEd9adL"
      },
      "source": [
        "# @tf.function\n",
        "def fit_train_model(model, epochs=10):\n",
        "    # tf.reset_default_graph()\n",
        "    center = None\n",
        "    w = Variable(10, name=\"w\", dtype=tf.float32)\n",
        "    b = Variable(-5, name=\"b\", dtype=tf.float32)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      inputs = random_batch()\n",
        "      # print(f\"Input shape = {inputs.shape}\")\n",
        "      \n",
        "      with tf.GradientTape(persistent=False) as tape:\n",
        "          model_outputs = model(inputs)\n",
        "          # print(f\"model output shape = {model_outputs[0].shape}\")\n",
        "          out = model_outputs[0]\n",
        "          # print(out.shape, type(out))\n",
        "          # embedded = model_outputs[-1]  # the last ouput is the embedded d-vector\n",
        "          # norm_embedded = normalize(embedded)\n",
        "          N = out.shape[1]\n",
        "          M = out.shape[2]\n",
        "          P = out.shape[0]\n",
        "          # reshaped_lstm_embedding = tf.reshape(out, shape=[out.shape[1], out.shape[2], out.shape[0]])\n",
        "\n",
        "          reshaped_lstm_embedding = tf.reshape(out, shape=[N, M, P])\n",
        "          # print(reshaped_lstm_embedding.shape)\n",
        "\n",
        "          embedded_split = reshaped_lstm_embedding\n",
        "          # print(f\"Center = {center}\")\n",
        "          if True:#center is None:\n",
        "            center = normalize(tf.reduce_mean(embedded_split, axis=1))              # [N,P] normalized center vectors eq.(1)\n",
        "            center_except = normalize(tf.reshape(tf.reduce_sum(embedded_split, axis=1, keepdims=True)\n",
        "                                                - embedded_split, shape=[N*M,P]))  # [NM,P] center vectors eq.(8)\n",
        "            # make similarity matrix eq.(9)\n",
        "            S = tf.concat(\n",
        "                [tf.concat([tf.reduce_sum(center_except[i*M:(i+1)*M,:]*embedded_split[j,:,:], axis=1, keepdims=True) if i==j\n",
        "                            else tf.reduce_sum(center[i:(i+1),:]*embedded_split[j,:,:], axis=1, keepdims=True) for i in range(N)],\n",
        "                          axis=1) for j in range(N)], axis=0)\n",
        "            # print(f\"1 S = {S.shape}\")\n",
        "          # else :\n",
        "          #   # If center(enrollment) exist, use it.\n",
        "          #   S = tf.concat(\n",
        "          #       [tf.concat([tf.reduce_sum(center[i:(i + 1), :] * embedded_split[j, :, :], axis=1, keepdims=True) for i\n",
        "          #                   in range(N)],\n",
        "          #                 axis=1) for j in range(N)], axis=0)\n",
        "          #   print(f\"2 S = {S.shape}\")\n",
        "\n",
        "          S = tf.abs(w)*S+b   # rescaling\n",
        "          sim_matrix = S\n",
        "\n",
        "\n",
        "          # sim_matrix = calc_similarity(out, w, b, N, M, P)\n",
        "          # print(\"similarity matrix size: \", sim_matrix.shape)\n",
        "\n",
        "          S_correct = tf.concat([S[i * M:(i + 1) * M, i:(i + 1)] for i in range(N)], axis=0)\n",
        "          # print(f\"S_correct = {S_correct.shape}\")\n",
        "          \n",
        "          tf_log = tf.math.log(tf.reduce_sum(tf.exp(S), axis=1, keepdims=True) + 1e-6)\n",
        "          # print(f\"tf log = {tf_log.shape}\")\n",
        "          tf_correct = S_correct - tf_log\n",
        "          total = -tf.reduce_sum(tf_correct)\n",
        "\n",
        "          loss = total #calculate_loss(sim_matrix, type=config[\"loss\"])\n",
        "\n",
        "          m_gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "          model_optimizer.apply_gradients(zip(m_gradients, model.trainable_variables))\n",
        "            \n",
        "      print(f'Epoch{epoch} - loss = {loss}...')\n"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmrrYHh09hVu",
        "outputId": "80fdbce9-a362-406c-8e4f-2897624e95c3"
      },
      "source": [
        "fit_train_model(model, epochs=100)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch0 - loss = 4201.73193359375...\n",
            "Epoch1 - loss = 2998.325927734375...\n",
            "Epoch2 - loss = 2500.6650390625...\n",
            "Epoch3 - loss = 2847.919677734375...\n",
            "Epoch4 - loss = 2867.009765625...\n",
            "Epoch5 - loss = 2819.614990234375...\n",
            "Epoch6 - loss = 2748.192626953125...\n",
            "Epoch7 - loss = 2638.098876953125...\n",
            "Epoch8 - loss = 2721.247314453125...\n",
            "Epoch9 - loss = 2725.575439453125...\n",
            "Epoch10 - loss = 2633.799560546875...\n",
            "Epoch11 - loss = 2681.768310546875...\n",
            "Epoch12 - loss = 2648.532958984375...\n",
            "Epoch13 - loss = 2597.80224609375...\n",
            "Epoch14 - loss = 2593.49462890625...\n",
            "Epoch15 - loss = 2468.27880859375...\n",
            "Epoch16 - loss = 2609.0654296875...\n",
            "Epoch17 - loss = 2574.455078125...\n",
            "Epoch18 - loss = 2588.8623046875...\n",
            "Epoch19 - loss = 2556.059814453125...\n",
            "Epoch20 - loss = 2559.06103515625...\n",
            "Epoch21 - loss = 2428.72509765625...\n",
            "Epoch22 - loss = 2551.76123046875...\n",
            "Epoch23 - loss = 2526.975341796875...\n",
            "Epoch24 - loss = 2472.829345703125...\n",
            "Epoch25 - loss = 2539.287841796875...\n",
            "Epoch26 - loss = 2529.3779296875...\n",
            "Epoch27 - loss = 2515.85400390625...\n",
            "Epoch28 - loss = 2448.75830078125...\n",
            "Epoch29 - loss = 2502.6767578125...\n",
            "Epoch30 - loss = 2492.138427734375...\n",
            "Epoch31 - loss = 2490.5966796875...\n",
            "Epoch32 - loss = 2452.344482421875...\n",
            "Epoch33 - loss = 2508.328369140625...\n",
            "Epoch34 - loss = 2295.236572265625...\n",
            "Epoch35 - loss = 2512.389404296875...\n",
            "Epoch36 - loss = 2495.765869140625...\n",
            "Epoch37 - loss = 2480.89306640625...\n",
            "Epoch38 - loss = 2478.25244140625...\n",
            "Epoch39 - loss = 2378.35595703125...\n",
            "Epoch40 - loss = 2482.68017578125...\n",
            "Epoch41 - loss = 2476.8125...\n",
            "Epoch42 - loss = 2464.9580078125...\n",
            "Epoch43 - loss = 2468.65625...\n",
            "Epoch44 - loss = 2450.15625...\n",
            "Epoch45 - loss = 2477.87353515625...\n",
            "Epoch46 - loss = 2447.66259765625...\n",
            "Epoch47 - loss = 2456.220703125...\n",
            "Epoch48 - loss = 2461.02392578125...\n",
            "Epoch49 - loss = 2469.0107421875...\n",
            "Epoch50 - loss = 2471.132568359375...\n",
            "Epoch51 - loss = 2453.833984375...\n",
            "Epoch52 - loss = 2452.45556640625...\n",
            "Epoch53 - loss = 2405.9404296875...\n",
            "Epoch54 - loss = 2283.23828125...\n",
            "Epoch55 - loss = 2453.075927734375...\n",
            "Epoch56 - loss = 2441.07861328125...\n",
            "Epoch57 - loss = 2447.505859375...\n",
            "Epoch58 - loss = 2448.177734375...\n",
            "Epoch59 - loss = 2454.887451171875...\n",
            "Epoch60 - loss = 2438.63623046875...\n",
            "Epoch61 - loss = 2441.24560546875...\n",
            "Epoch62 - loss = 2400.232421875...\n",
            "Epoch63 - loss = 2445.78515625...\n",
            "Epoch64 - loss = 2419.21728515625...\n",
            "Epoch65 - loss = 2440.0263671875...\n",
            "Epoch66 - loss = 2450.779296875...\n",
            "Epoch67 - loss = 2446.123291015625...\n",
            "Epoch68 - loss = 2449.7412109375...\n",
            "Epoch69 - loss = 2445.823974609375...\n",
            "Epoch70 - loss = 2405.1181640625...\n",
            "Epoch71 - loss = 2444.4404296875...\n",
            "Epoch72 - loss = 2420.748779296875...\n",
            "Epoch73 - loss = 2405.050048828125...\n",
            "Epoch74 - loss = 2406.73974609375...\n",
            "Epoch75 - loss = 2410.49658203125...\n",
            "Epoch76 - loss = 2434.830078125...\n",
            "Epoch77 - loss = 2424.94775390625...\n",
            "Epoch78 - loss = 2360.74951171875...\n",
            "Epoch79 - loss = 2335.1787109375...\n",
            "Epoch80 - loss = 2431.8291015625...\n",
            "Epoch81 - loss = 2371.941162109375...\n",
            "Epoch82 - loss = 2399.114990234375...\n",
            "Epoch83 - loss = 2435.527587890625...\n",
            "Epoch84 - loss = 2436.54150390625...\n",
            "Epoch85 - loss = 2433.9638671875...\n",
            "Epoch86 - loss = 2433.5966796875...\n",
            "Epoch87 - loss = 2437.979736328125...\n",
            "Epoch88 - loss = 2440.409423828125...\n",
            "Epoch89 - loss = 2433.1220703125...\n",
            "Epoch90 - loss = 2430.072265625...\n",
            "Epoch91 - loss = 2374.171875...\n",
            "Epoch92 - loss = 2421.828857421875...\n",
            "Epoch93 - loss = 2433.0361328125...\n",
            "Epoch94 - loss = 2430.462158203125...\n",
            "Epoch95 - loss = 2403.76220703125...\n",
            "Epoch96 - loss = 2428.4892578125...\n",
            "Epoch97 - loss = 2401.61279296875...\n",
            "Epoch98 - loss = 2425.6787109375...\n",
            "Epoch99 - loss = 2422.92626953125...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rory05P39ofi"
      },
      "source": [
        "# (20, 40, 174)\n",
        "# Center = None\n",
        "# 1 S = (800, 20)\n",
        "# similarity matrix size:  (800, 20)\n",
        "# S_correct = (800, 1)\n",
        "# tf log = (800, 1)\n",
        "\n",
        "# 1 S = (800, 20)\n",
        "# similarity matrix size:  (800, 20)\n",
        "# S_correct = (800, 1)\n",
        "# tf log = (800, 1)\n",
        "\n",
        "# Epoch0 - loss = 3105.39892578125...\n",
        "\n",
        "# Epoch1 - loss = 2931.333984375..."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}